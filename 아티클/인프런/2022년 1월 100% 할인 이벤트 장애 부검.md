## 아티클 링크

https://tech.inflab.com/202201-event-postmortem/
## 세줄 요약

## 내용

### 발단

#### 2022 년 특정 강의 100% 할인 이벤트 진행

- 강의 125개 를 모두 0원에 기간동안 등록 가능한 이벤트
- 1차 : 2022년 1월 3일 ~ 2022년 1월 17일
- 2차 : 2022년 1월 18일 ~ 2022년 1월 24일
- 트위터 및 각종 커뮤니티에서도 공유가 되며 평소를 뛰어넘는 트래픽 발생 (5배~20배) !

7일 간 , 76,730 의 신규 가입자 발생! ( 전년 대비 1,885 % )
![500](https://i.imgur.com/SypjxT0.png)

=> 트래픽이 뛰며 장애가 지속적 발생

![500](https://i.imgur.com/EOyJaYj.png)

- 4일 간 , 오전 9시 & 오후 10시 ( 피크 타임 ) 되면 서비스가 불가능한 상태!
##### 의문점

- 폭증한 트래픽이더라도 , 쿠팡 / 배민 등 비하면 작은 트래픽인데 , Scale Up / Out 으로 대비를 안했나?
- 이벤트 준비하며 성능 테스트도 안했나?
- 1월 8일 장애 이후 17일 까지는 10일 간 시간이 있는데 , 대응책이 없었나?
- 사담 : 접속 대기를 걸어서 , 부하를 방지 하는 형식을 쓸 수는 없었나?

### 장애 회고

### 2-1. 인프라
![500](https://i.imgur.com/Oljj9Vb.png)

- ELB , ElasticCache , DynamoDB , AWS Lambda 등은 이번 장애와 무관이므로 제외

- 현재 , 모놀리틱 NodeJS 프로젝트를 ECS Fargate 에 올려 트래픽 따라 10~30대 유지
- PostgreSQL 은 1대의 쓰기 - 2~7대의 읽기
	-> 조회용 Query 는 읽기 DB 통해 처리
	-> 등록 / 수정 / 삭제용 Query 는 쓰기 DB에 처리
( Multi Master 경우 고려 X , 서비스 별 별도 DB 구성이 더욱 가용성 및 성능에 효과적이라 생각하기 때문 )

=> 모든 서비스가 하나의 DB 이므로 , 한 서비스 장애는 전체 서비스의 장애로 확장!!

### 2-2. 1월 3일 장애

- 저녁 8시경 , 예상 못한 규모의 접속량 증가 발생!
- 오후 8:30 ~ 오후 10:00 까지 서비스가 사용 불가능할 정도로 지연 현상 발생
##### 강의 페이지 관심회사 조회 기능에서 발생한 슬로우 쿼리!

![400](https://i.imgur.com/TEEUYE0.png)

- 해당 API 는 실시간 가공 처리 로직 -> 인덱스 적용 안된 경우 성능 부하 유발 가능!
	( 인덱스가 없으므로 , 하나씩 순회하며 탐색을 해나가야 하므로 )
- 모든 강의 소개 페이지에서 호출하는 쿼리이므로 , 지연이 되며 Node Process 의 Connection이 빠르게 고갈 되어감
- 이후 요청들은 모두 대기 상태로 빠짐 -> 일정 시간 대기 후 Node Process 들이 강제 종료
- ECS 설정 통해 빠진 Process 만큼 다시 복구는 되나 , API 외 처리하는 다른 쿼리들 역시 전부 취소
=> 서비스가 전체적 사용 불가능한 상태에 빠진다!!

- 관심 회사 조회 API Query 중 , 인덱스 적용 안된 것을 로그와 실행계획 통해 확인!
=> 인덱스를 추가함으로 , 해당 문제 일단락!!!
##### 쓰기 DB CPU 과부하

![500](https://i.imgur.com/qiO1NZn.png)

- 본격적인 트래픽이 유입!
- 오후 11:30분 부터 1월 4일 새벽 01:30 까지 2시간 동안 쓰기 DB 의 CPU 가 90% 이상 되며,
	전체 서비스가 사용 불가능!

하단 , 두가지 문제에서 함께 발생!
##### Redis 부하 감소를 위한 DB 로 분산시킨 Cache 용 Data

- 모든 Cache 데이터를 Redis 에서 관리하며 , Redis 에서 부하 문제가 종종 발생
	=> 이를 위해 , 일부 Cache Data 는 DB에서 관리하도록 구성

그러나 , 해당 과정에서 초기 Cache Data 설계를 잘못해 1개의 JSON 에서 모든 Cache Data 관리!! ( WOW 🫢 )

적재된 고용량 캐시 Row 1건이 캐시 초기화 / 재생성 행위가 발생할 때마다 쓰기 DB에 큰 부하를 발생
( 모든 캐시 데이터가 1건에 다 담겨 있으므로 )
트래픽 증가에 따라 , 캐시 초기화 & 재생성이 빈번하게 발생되며 문제 발생!
=> 1개 Row 에 대한 요청이 겹치며 DeadLock 이슈로 확장!

![500](https://i.imgur.com/8RCbnZL.png)
##### 쓰기 DB에서 호출되던 조회용 Query

- 읽기 DB로 전환 못한 조회용 Query 들 역시 쓰기 DB 가 담당하다보니 , 부하 가속화!
> 인프런은 Aurora DB 를 작년 여름 도입하여 쓰기 / 읽기 DB 분산이 100% 적용되지 않은 상태
#### 해결책

- 당장 캐시 데이터 해체하기에는 조치하기 어려움!

하단 , 2가지 작업 우선 진행
- 캐시 초기화 & 재생성 주기 변경
- 쓰기 DB 부하를 주는 조회용 Query 만 먼저 읽기 DB로 전환

=> 3일의 장애를 일단락 하고 , 새벽 1시가 넘은 시간이라 우선 퇴근

### 2-3 .1월 4일 장애

1월 4일 오전 9시가 되자 , 다시 한번 트래픽 증가 ( 약속의 시간.. )
> 오전 / 오후 모두 발생했으나 , 둘 다 테이블 관련 장애이므로 하나로 묶어서 설명

9시부터 ~ 12시 까지 총 3시간 동안 서비스를 사용할 수 없게 됌!

#### 강의 조회 관련 슬로우 쿼리

![500](https://i.imgur.com/eZzwdSS.png)

- 상위 9개 Query 가 강의 조회
##### Why?

![400](https://i.imgur.com/GsFVkcm.png)

- 실제 강의 검색 뿐 아니라 , 추천형 기능들이 모두 강의 테이블들을 기반으로 작동

=> 슬로우 쿼리 종류가 워낙 다양하므로 , 한번에 모든 쿼리 수정은 불가능이라 판단!

당장 문제 해결 위해 , r5.8xlarge 8대로 늘려 시간을 벌기로 결정! ( Scale Up/Out ) - On Demand 로 월 4천만원의 비용 발생!!
=> 몇십분 정도는 시간을 벌어주나 , 장애 해소에 전혀 도움이 되지 못함!

모니터링 하며 , 나온 2가지 문제를 빠르게 수정!
#### 문제
##### 비효율적인 실행 계획
슬로우 쿼리들의 대부분은 테이블 컬럼 변조 ( lower 사용해 변환 ) 하거나 부정조건 ( != , NOT IN ) 처럼
	인덱스를 사용할 수 없는 쿼리들로 되어 있었음!
=> 테이블 풀 스캔 쿼리를 동시 수십 ~ 수백개를 발생 시킴!!
##### 불필요한 조회항목들
- 일부 슬로우 쿼리들이 select * 로 되어있어 사용 여부 관계없이 모든 컬럼 조회 유도
	-> 사용하지 않는 대용량 컬럼들 역시 함께 조회
	![300](https://i.imgur.com/QMic2yg.png)
#### 해결
##### 조회 쿼리 개선
- 인덱스를 추가하거나 쿼리 변경해 인덱스를 사용하도록 개선
- 개선인 안될거 같으면 차라리 기능을 OFF!!
- select * 로 조회되던 컬럼에서 고용량 강의 소개 컬럼은 모두 제거
	( 실제 기능에서 사용되지 않던 컬럼 )

### 수정 지연

모니터링 & 슬로우쿼리 알람을 통해 문제 되는 쿼리들을 조사하는데는 30분 안되는 짧은 시간 소요!
	-> 실제 해결까지는 3시간 소용?!!

#### 무분별한 쿼리 추상화로 인해 지연
![500](https://i.imgur.com/ZZu3Mh8.png)
( 솔직히 이건 무슨 코드인지 감도 안온다...)

- 모든 쿼리가 함수 / Hook / Custom Event 사용해 구성하다보니 , 해당 쿼리의 실제 코드 찾는데 많은 시간 소모!
- 기존 프로젝트에는 타입 & 테스트 코드가 없으므로 안정감 하락 & 부담감 상승
=> 로컬에서 충분한 수동 테스트를 하고 나서야만 실제 배포가 가능했음! ( 장애 시간이 더 길어지게 유발 )

위 조치 사항을 진행하며 , 서비스가 안정화 되는 걸 확인 후 퇴근 가능!!

### 2.4 1월 7일 장애

1월 4일이 지나고 1월 5일 & 6일에는 더 이상 장애 발생! X ( The End?!! )
이벤트를 통한 장애 현상은 모두 해소되었다고 안심!

귀신 같이 1월 7일 저녁 9시부터 여러 지연 현상 발생!
=> 4일과 마찬가지로 , 강의 테이블의 쿼리가 문제!

##### where id in (ID) Query
- ID 의 개수가 100개를 초과할 시 , 인덱스를 사용하지 못함
	-> 테이블 풀 스캔을 유발!!

- 기존에는 , 문제 없었으나 125개의 강의 100% 할인하는 것을 통한 문제 발생!
	-> 어짜피 공짜이므로 , 모두 담아서 일괄 결제 급증!!

=> 해당 문제 해결 위해서 "한번에 담는 강의 수 제한 " 보다는 IN 절에 담는 ID 를 20개씩 끊어서
	Promise.all 로 분할처리하도록 구성해 IN절 통한 인덱스 효과 다시 사용하도록 함


### 2-5. 1월 17일 장애 (최종)

#### Before 1-17
- 첫 주 할인 이벤트를 끝내고 , 17일 부터 시작되는 두번째 이벤트를 맞이하기 위한 준비 시작

가장 큰 문제는 성능 테스트를 할 수 없다..!

선착순 할인 , 선착순 쿠폰 등 특정 페이지,API 에서 트래픽 예상되고 , 이를 성능 테스트 하는게 일반적
첫 주 이벤트 장애 회고해보면 어느 페이지 문제가 될지 전혀 예상 불가능!
( 강의소개 , 추천 강의 , 메인 페이지 , 수강 바구니 , 결제 페이지 , 관심회사 등등 )
=> 대부분의 페이지가 , 트래픽이 몰리면 언제든 장애가 날 수 있는 시한폭탄과 같다!!

- 가장 예상되는 지점인 강의 테이블이 있으나 , 수많은 기능들이 연관 되어 있어 1주일 이라는 시간으로는 불가능!
##### 보편적인 성능 개선
- PostgreSQL 버전 업데이트 ( 10.16 -> 11.13 )
	-> B Tree Index 성능 개선 & B Tree Index Covering Index 효과 기대
- Query Timeout 설정 ( 5000 ms )
	-> 5초이상 수행되는 Query 일 시 , 강제 종료 해 Long Query 자체 사전 차단
- Max Connection 증설
- 2초 이상 수행되는 Query 들에 대한 성능 개선
![500](https://i.imgur.com/dIro14y.png)

- 1월 15일에도 , 엄청난 트래픽이 발생했으나 , 서비스에 이슈가 없었음
=> 해치웠나..? + 장기 개선 계획을 준비해야겠다.

#### 1월 17일
는 , 본격 이벤트 시작된 오후 10시부터 RDS 의 수치들이 요동치기 시작!

![500](https://i.imgur.com/rWdhFvM.png)

- 1월 17일 오후 10시 30분 , 1월 6번째 서비스 장애 시작

##### 근본 원인 분석
- 1만건이 되지 않는 강의 테이블만 탐색하는 쿼리에서도 슬로우가 발생!
	( 10억건이 넘는 테이블에서도 인덱스 설계를 통해 , 1~2초 내 쿼리 수행이 가능! )
	=> 쓰기 DB 의 Table 의 Tuple 정보 확인
![500](https://i.imgur.com/7hIEmre.png)

- Dead Tuple 의 Ratio 가 99% ( Auto Vacuum 을 뛰어넘겨버림 )
=> 수동 vacuum 수행해 Dead Tuple 처리

![500](https://i.imgur.com/wqlY3e8.png)

- 수강생을 증가시키는 , Update 쿼리가 수강 신청 이벤트 발생시 항상 수행
	=> 강의 테이블 병목을 시키는 근본 원인!
- PostgreSQL 만의 MVCC 때문에 Tuple 들은 Dead 로 처리되어 데이터가 보관되어 있음
	( 이벤트가 많아질수록 Disk I/O 증가로 성능 저하! )

특히 , 강의 테이블은 고용량 컬럼들로 인해 Dead Tuple 하나 하나 용량이 매우 높음!
	-> 잦은 업데이트 쿼리는 고용량 Dead Tuple 대량 생산
##### 정리
- 100개 이상 강의 일괄 등록하는 수강생의 급증
- 수강생수 업데이트 위해 고용량 컬럼 가진 강의 테이블의 잦은 업데이트 발생
- Dead Tuple 의 폭발적 증가로 모든 강의 테이블 쿼리들 병목 현상
=> 병목이 , 읽기 DB 슬로우 쿼리단에서만 보여주고 있어 , 읽기 DB 만을 모니터링 한 것이 근본 원인 파악하는데 방해!
=> 고용량 컬럼과 수강생 수 업데이트 를 분리하자

- 다른 기능들과 영향도 파악해서 수강생 수 컬럼 분리 보단 , 강의 소개 컬럼 분리를 선택
	=> 이벤트로 인한 장애 더이상 발생 X !!

### 마무리
#### 해당 이벤트를 통해 주어진 숙제
- 모든 영역에서 높은 장애 민감도
- 한 테이블 장애가 전체 서비스의 장애로 확장 되는 구조 - 모놀리식
- 성능 개선이 쉽지 않은 테이블 , 캐시 구조
- 직관적이지 않은 SQL 코드
- 안정감을 주지 못하는 코드 ( Typeless , 테스트 코드 부재 )
- 안정적인 Batch 환경의 부재
- 아직 밝혀지지 않은 수많은 숨겨진 장애 포인트

#### MSA?

- 인프런 서비스는 하나의 거대한 모놀리틱 프로젝트로 3년간 개발이 되어옴
	-> MSA 를 떠올릴 수도 있다!
- 앞선 회사들의 경험을 고려할 떄 MSA 가 최선책이라고 생각하지 않음!
	-> 섣부른 도입이 오히려 높은 복잡도로 큰 문제를 일으킬 수도 있으므로
	( 배달의 민족도 심지어 3년간 수백명의 개발자들과 함께 진행 )
=> 적정선의 모놀리틱을 유지하며 , 장애 전파율이 높은 일부 도메인들은 별도로 분리해 적정 수준 경계를 둘 예정

## 결론

- 이벤트는 개발자들의 눈물로 이루어진 거 같다..
	->단순히 , 이벤트를 즐길 때는 몰랐으나 , 증가하는 트래픽에 인한 문제 발생은 온전히 개발자의 몫,,

- 몹시도 중요한 DB 설계
	-> 결국 대부분의 에러는 DB 단위에서부터 발생
	-> 특히 , 모놀리틱이므로 특정 부분이 DB로 인한 에러 발생 시 전체 에러 전파 !!

- MSA 가 최고는 아니다.
	-> 처음부터 MSA 를 설계한 것이 아니라면 , MSA 전환기는 몹시 험하다..!
	( 하루아침에 뚝딱 되는 게 아닌걸 명심하자 )

## 참고 링크

[배민의 PosgreSQL 설명](https://techblog.woowahan.com/6550/)
##### Writed By Obisidan